{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "#Data pre processing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Scoring\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#Hyperparam opti\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "#Mlflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=AttributeError)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_global_variable(variable_name):\n",
    "    \"\"\"\n",
    "    Check if a global variable exists and delete it if it does.\n",
    "    \"\"\"\n",
    "    for v in variable_name:\n",
    "        if v in globals():\n",
    "            # Delete the global variable\n",
    "            del globals()[v]\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def optimize_int_memory(df:pd.DataFrame):\n",
    "    # Step 1: Refactor dtypes to save space\n",
    "    print(df.dtypes.value_counts())\n",
    "    initial_memory_usage = df.memory_usage(deep=True).sum()\n",
    "    print(f\"Initial memory usage: {initial_memory_usage / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Check each int64 column\n",
    "    for col in df.select_dtypes(include=['int64','Int64']).columns:\n",
    "        max_val = df[col].max()\n",
    "        min_val = df[col].min()\n",
    "        \n",
    "        if min_val >= 0:\n",
    "            if max_val <= 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif max_val <= 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "            elif max_val <= 4294967295:\n",
    "                df[col] = df[col].astype('uint32')\n",
    "        else:\n",
    "            if min_val >= -128 and max_val <= 127:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif min_val >= -32768 and max_val <= 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            elif min_val >= -2147483648 and max_val <= 2147483647:\n",
    "                df[col] = df[col].astype('int32')\n",
    "    \n",
    "    \n",
    "    print(df.dtypes.value_counts())\n",
    "    final_memory_usage = df.memory_usage(deep=True).sum()\n",
    "    print(f\"Final memory usage: {final_memory_usage / 1024**2:.2f} MB\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dataset_version(dataset_version : Literal['1.0','1.1','2.0','3.0']):\n",
    "    \"\"\"\n",
    "    1.0 : Full dataset from our data kernel with Na\n",
    "    1.1 : Full dataset from our data kernel `without` Na (imputed by mean)\n",
    "    2.0 : sampled data, by default 10% of the original data\n",
    "    2.1 : sampled data, by default 10% of the original data `without` Na (imputed by mean)\n",
    "    3.0 : scaled data without Na by default StandardScaler \n",
    "    3.1 : scaled and sampled data without Na, StandardScaler and 10%\n",
    "    4.0 : scaled and PCA\n",
    "    4.1 : scaled and PCA and sampled, by default 10% of the original data size \n",
    "\n",
    "    # How to use:\n",
    "    `X, y, current_version = change_dataset_version('1.0')`\n",
    "    \"\"\"\n",
    "    reset_global_var = ['X','y','current_version']\n",
    "    clear_global_variable(reset_global_var)\n",
    "\n",
    "    if dataset_version == '1.0':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        for col in X.select_dtypes(include=['Int64']).columns:\n",
    "            X[col] = X[col].astype('float64')\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '1.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "        \n",
    "        # Replace negative infinity values (if applicable)\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        #Identify int col:\n",
    "        int_columns = train_data.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        #round the mean for those columns:\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        train_data = optimize_int_memory(train_data)\n",
    "        \n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '2.0':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Sample 10% of the data\n",
    "        train_data = train_data.sample(frac=0.10, random_state=42)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        for col in X.select_dtypes(include=['Int64']).columns:\n",
    "            X[col] = X[col].astype('float64')\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '2.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Sample 10% of the data\n",
    "        train_data = train_data.sample(frac=0.10, random_state=42)\n",
    "\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "\n",
    "        # Replace negative infinity values\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        #Identify int col:\n",
    "        int_columns = train_data.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        #round the mean for those columns:\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        #Restructure the dtypes for memory optimisation\n",
    "        train_data = optimize_int_memory(train_data)\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    elif dataset_version == '3.0':\n",
    "        # Same process as '1.0', but with additional scaling step.\n",
    "        # Load the data.\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X.\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "\n",
    "        # Scaling the data.\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled_array = scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(X_scaled_array, columns=X.columns)\n",
    "\n",
    "        del train_data, X_scaled_array\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '3.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Sample 10% of the data\n",
    "        train_data = train_data.sample(frac=0.10, random_state=42)\n",
    "\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "\n",
    "        # Replace negative infinity values\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        #Identify int col:\n",
    "        int_columns = train_data.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        #round the mean for those columns:\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        #Restructure the dtypes for memory optimisation\n",
    "        train_data = optimize_int_memory(train_data)\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        \n",
    "        # Scaling the data.\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled_array = scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(X_scaled_array, columns=X.columns)\n",
    "\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data, X_scaled_array\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '4.0':\n",
    "        # Same process as '3.0', but with additional PCA step.\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X.\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "\n",
    "        # Replace negative infinity values\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        #Identify int col:\n",
    "        int_columns = train_data.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        #round the mean for those columns:\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        #Restructure the dtypes for memory optimisation\n",
    "        train_data = optimize_int_memory(train_data)\n",
    "\n",
    "        # Scaling the data.\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # Apply PCA.\n",
    "        pca = PCA(n_components=0.95)\n",
    "        X = pca.fit_transform(X)\n",
    "\n",
    "        # Convert back to DataFrame.\n",
    "        X = pd.DataFrame(X, columns=[\"PC\" + str(i) for i in range(1, X.shape[1] + 1)])\n",
    "\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    if dataset_version == '4.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index', 'level_0']\n",
    "        # Get the columns to be included in X.\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "\n",
    "        y = train_data.pop('TARGET')\n",
    "        X = train_data\n",
    "\n",
    "        # Data splitting\n",
    "        X, _, y, _ = train_test_split(X, y, stratify=y, test_size=0.9, random_state=42)\n",
    "    \n",
    "        # Replace infinity values more efficiently\n",
    "        inf_mask = X == np.inf\n",
    "        max_values = X[~inf_mask].max()\n",
    "        X.where(~inf_mask, max_values, axis=1, inplace=True)\n",
    "\n",
    "        minus_inf_mask = X == -np.inf\n",
    "        min_values = X[~minus_inf_mask].min()\n",
    "        X.where(~minus_inf_mask, min_values, axis=1,inplace=True)\n",
    "\n",
    "        # Fill NaN values with column means\n",
    "        int_columns = X.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        column_means = X.mean()\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        X.fillna(column_means, inplace=True)\n",
    "\n",
    "        # Memory optimization\n",
    "        X = optimize_int_memory(X)\n",
    "        \n",
    "\n",
    "        # Scaling\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # PCA\n",
    "        pca = PCA(n_components=0.95)\n",
    "        X = pca.fit_transform(X)\n",
    "        X = pd.DataFrame(X, columns=[\"PC\" + str(i) for i in range(1, X.shape[1] + 1)])\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    ratio = y.sum()/len(y)*100\n",
    "    print(\"Target 1-0 ratio: {:.2f}%\".format(ratio))\n",
    "    return X, y, dataset_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_5_rows(X, y):\n",
    "\n",
    "    X_5 = X.head()\n",
    "    y_5 = y.head()\n",
    "\n",
    "    X_5.to_csv('./test_df/X_head', index = False)\n",
    "    y_5.to_csv('./test_df/y_head',index = False)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create functions that will become our baseline for model iteration**\n",
    "\n",
    "After we have our data we will put it through this function and it will get us:\n",
    "- The best hyperparameters for those data and this model\n",
    "- Save all information about the model run so we can compare with other results and pick the best model at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'randomForestClassifier': lambda params: RandomForestClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42),\n",
    "    'lightGBM': lambda params: lgb.LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], n_estimators=params['n_estimators'], random_state=42),\n",
    "    'logisticRegression': lambda params: LogisticRegression(C=params['C'],\n",
    "                                                            penalty=params.get('penalty', 'l2'),\n",
    "                                                            fit_intercept=params.get('fit_intercept', True), \n",
    "                                                            solver=params.get('solver', 'lbfgs'),\n",
    "                                                            l1_ratio=params.get('l1_ratio', None),  \n",
    "                                                            random_state=42, \n",
    "                                                            max_iter=1500),\n",
    "    'dummyClassifier': lambda params: DummyClassifier(strategy='constant', constant=0),\n",
    "    'fullLGBM' : lambda params: lgb.LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], n_estimators=params['n_estimators'], random_state=42),\n",
    "    #'SVC': lambda params: SVC(C=params['C'], gamma=params['gamma'], probability=True, random_state=42)\n",
    "    # Add more models here as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_name, params, X, y, use_smote = False):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    accuracies = []\n",
    "    adjusted_scores = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    all_y_pred_prob = []  \n",
    "    all_y_true = []\n",
    "\n",
    "\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "\n",
    "        if use_smote:\n",
    "            #smote = SMOTE()\n",
    "            smote = BorderlineSMOTE(kind='borderline-1')\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "            #ratio = y_train.sum()/len(y_train)*100\n",
    "            #print(\"Target 1-0 ratio: {:.2f}%\".format(ratio))\n",
    "            \n",
    "        #Create the model with given param\n",
    "        model = models[model_name](params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        #calculate the pred probabilities on the test set for ROC AUC later\n",
    "        y_pred_prob = model.predict_proba(X_val)[:,1]\n",
    "\n",
    "        #Calculate accuracy\n",
    "        y_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        #Calculate recall and f1_score\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        #accuracy is not everything, in our case errors on FP and FN are vastly different FN are lossing the bank much more money than FP\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "        adjusted_score = 10 * fn + fp\n",
    "        adjusted_scores.append(adjusted_score)\n",
    "\n",
    "        all_y_pred_prob.extend(y_pred_prob)\n",
    "        all_y_true.extend(y_val)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_adjusted_score = np.mean(adjusted_scores)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    return model, mean_accuracy, all_y_true, all_y_pred_prob, mean_recall, mean_f1_score, adjusted_scores, mean_adjusted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, params, X, y, use_smote : bool):\n",
    "    \"\"\"\n",
    "    This function is what optuna will try to minimze during the best param search \n",
    "    \"\"\"\n",
    "    _, _, _, _, _, _, _, adjusted_score= train_and_evaluate_model(model_name, params,X,y,use_smote)\n",
    "    \n",
    "\n",
    "    return adjusted_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_experiment(model_name : Literal['lightGBM','randomForestClassifier','logisticRegression','dummyClassifier'],\n",
    "                           X, y, max_trials:int, use_smote = False):\n",
    "    \n",
    "    #define the pruner\n",
    "    pruner = MedianPruner(n_startup_trials=1, n_warmup_steps=5, interval_steps=1)\n",
    "    # Create an Optuna study object\n",
    "    study = optuna.create_study(direction='minimize', pruner=pruner)  # 'minimize' for custom \"mÃ©tier\" score\n",
    "\n",
    "    # Optimize the objective function (number of trials specified by max_trials)\n",
    "    study.optimize(lambda trial: objective(model_name, \n",
    "                                       {'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                                        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "                                       } if model_name == 'randomForestClassifier' else\n",
    "\n",
    "                                       {'num_leaves': trial.suggest_int('num_leaves', 31, 150),\n",
    "                                        'max_depth': trial.suggest_int('max_depth', -1, 50),\n",
    "                                        'n_estimators': trial.suggest_int('n_estimators', 100, 600)\n",
    "                                       } if model_name == 'lightGBM' else\n",
    "                                       {\n",
    "                                        'C': trial.suggest_float('C', 0.001, 100),\n",
    "                                        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet', 'none']),\n",
    "                                        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "                                        'solver': trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "                                        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0) if trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet', 'none']) == 'elasticnet' else None\n",
    "                                        } if model_name == 'logisticRegression' else\n",
    "                                       {   \n",
    "                                       }if model_name == 'dummyClassifier'else\n",
    "                                        # Add more hyperparameters as needed\n",
    "                                       {\n",
    "                                        #Write Hparams for other models\n",
    "                                       }, \n",
    "                                       X, y, use_smote), n_trials=max_trials)\n",
    "\n",
    "    # Retrieve the best hyperparameters from the study\n",
    "    best_params = study.best_params\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_best_model(model_name : Literal['lightGBM','randomForestClassifier','logisticRegression','dummyClassifier'],\n",
    "                    best_params, X, y, dataset_version:str, use_smote = False, use_shap = False):\n",
    "    \n",
    "        # Start an MLflow run to track the training process\n",
    "    with mlflow.start_run():\n",
    "        # Log the dataset version\n",
    "        mlflow.log_param('dataset_version', dataset_version)\n",
    "        mlflow.log_param('SMOTE', use_smote)\n",
    "        mlflow.log_param('model type', model_name)\n",
    "\n",
    "        # Log the hyperparameters\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Train and evaluate the model using the best hyperparameters\n",
    "        model, accuracy, all_y_true, y_pred_prob, recall, f1_score, adjusted_scores, adjusted_score = train_and_evaluate_model(model_name, best_params, X, y, use_smote)\n",
    "\n",
    "        # Log the metrics\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        mlflow.log_metric('recall', recall)\n",
    "        mlflow.log_metric('f1_score', f1_score)\n",
    "        mlflow.log_metric('adjusted_score', adjusted_score)\n",
    "        for i, score in enumerate(adjusted_scores, 1):\n",
    "            mlflow.log_metric(f'adjusted_score_{i}', score)\n",
    "\n",
    "        # Save the model as an artifact\n",
    "        mlflow.sklearn.log_model(model, 'model')\n",
    "\n",
    "        #region ROC AUC\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(all_y_true, y_pred_prob)\n",
    "        mlflow.log_metric('roc_auc', roc_auc)\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(all_y_true, y_pred_prob)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        roc_curve_img_path = './Exports/roc_curve.png'\n",
    "        plt.savefig(roc_curve_img_path)\n",
    "\n",
    "        # Log the ROC curve image as an artifact\n",
    "        mlflow.log_artifact(roc_curve_img_path)\n",
    "\n",
    "        # Close the plot to free up memory\n",
    "        plt.close()\n",
    "        # endregion \n",
    "\n",
    "        #region Shap\n",
    "        if use_shap:            \n",
    "            #local importance:\n",
    "            index = 1\n",
    "\n",
    "            if model_name == 'dummyClassifier':\n",
    "                return  \n",
    "            elif model_name == 'logisticRegression':\n",
    "                explainer = shap.Explainer(model, X)\n",
    "                shap_values = explainer(X)\n",
    "                shap_html = shap.plots.force(shap_values[index], show=False)\n",
    "                html_filepath = f\"./Exports/shap_force_plot_{index}.html\"\n",
    "                shap.save_html(html_filepath, shap_html)\n",
    "\n",
    "            else: #TreeBased model\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X)\n",
    "                shap_html = shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X.iloc[0,:], show=False)\n",
    "                # Save the HTML content to a file\n",
    "                html_filepath = f\"./Exports/shap_force_plot_{index}.html\"\n",
    "                shap.save_html(html_filepath, shap_html)\n",
    "\n",
    "            # Now for the full model feature importance\n",
    "            shap.summary_plot(shap_values, X, show=False)\n",
    "            plt.savefig('./Exports/shap_summary.png')\n",
    "    \n",
    "            # Log the SHAP force plot and summary plot as artifacts\n",
    "            mlflow.log_artifact(html_filepath)\n",
    "            mlflow.log_artifact('./Exports/shap_summary.png')\n",
    "        #endregion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the experiment already exists\n",
    "experiment_id = mlflow.get_experiment_by_name('credit_models')\n",
    "if experiment_id is None:\n",
    "    mlflow.create_experiment('credit_models', artifact_location='./app/mlruns/1')\n",
    "    print('Experiment created')\n",
    "else:\n",
    "    print(f\"Experiment 'credit_models' already exists with id {experiment_id.experiment_id}.\")\n",
    "    mlflow.set_experiment('credit_models')\n",
    "\n",
    "mlflow.set_tracking_uri('sqlite:///app/mlflow.db')\n",
    "\n",
    "#To start the mlflow server to see the dashboard //// Be sure to change your directory to the app folder\n",
    "#mlflow server --backend-store-uri sqlite:///mlflow.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, current_version = change_dataset_version('4.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to save some data for the model you put in production to test it \n",
    "#save_5_rows(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optuna_params = run_optuna_experiment('logisticRegression',X,y,20,use_smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optuna_params = {'C': 13.488158255561652, 'penalty': 'none', 'fit_intercept': True, 'solver': 'lbfgs'}\n",
    "run_best_model('logisticRegression',best_optuna_params,X,y,current_version,use_smote=True,use_shap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "**Let's see how the predict part is going to look like**\n",
    "\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.head()\n",
    "y1 = y.head()\n",
    "\n",
    "#set the current model ID and load the model\n",
    "# e144dc8942db42d69aa7f5aebf036a01   fba7b8274f184f3ca41e1d6672eb2ab1\n",
    "model_id = 'e144dc8942db42d69aa7f5aebf036a01'\n",
    "model_uri = f\"./app/mlruns/1/{model_id}/artifacts/model\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel:\n",
    "    def __init__(self, mlflow_model):\n",
    "        self.model = mlflow_model\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(self.model._model_impl, \"predict_proba\"):\n",
    "            return self.model._model_impl.predict_proba(X)\n",
    "        else:\n",
    "            raise AttributeError(\"Underlying model does not have a 'predict_proba' method.\")\n",
    "\n",
    "wrapped_model = WrappedModel(model)\n",
    "proba = wrapped_model.predict_proba(X1)\n",
    "prediction = model.predict(X1)\n",
    "\n",
    "api_answers = []\n",
    "for i in range(0, len(prediction)):\n",
    "    api_answer = [prediction[i], proba[i][0], proba[i][1]]\n",
    "    api_answers.append(api_answer)\n",
    "display(api_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
