{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do before starting the notebook\n",
    "\n",
    "- Open a terminal in the app folder\n",
    "- Start the mlflow backend:\n",
    "    - mlflow server --backend-store-uri sqlite:///mlflow.db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "#Data pre processing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Scoring\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#Hyperparam opti\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "#Mlflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=AttributeError)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_global_variable(variable_name):\n",
    "    \"\"\"\n",
    "    Check if a global variable exists and delete it if it does.\n",
    "    \"\"\"\n",
    "    for v in variable_name:\n",
    "        if v in globals():\n",
    "            # Delete the global variable\n",
    "            del globals()[v]\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def optimize_int_memory(df:pd.DataFrame):\n",
    "    # Step 1: Refactor dtypes to save space\n",
    "    print(df.dtypes.value_counts())\n",
    "    initial_memory_usage = df.memory_usage(deep=True).sum()\n",
    "    print(f\"Initial memory usage: {initial_memory_usage / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Check each int64 column\n",
    "    for col in df.select_dtypes(include=['int64','Int64']).columns:\n",
    "        max_val = df[col].max()\n",
    "        min_val = df[col].min()\n",
    "        \n",
    "        if min_val >= 0:\n",
    "            if max_val <= 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif max_val <= 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "            elif max_val <= 4294967295:\n",
    "                df[col] = df[col].astype('uint32')\n",
    "        else:\n",
    "            if min_val >= -128 and max_val <= 127:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif min_val >= -32768 and max_val <= 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "            elif min_val >= -2147483648 and max_val <= 2147483647:\n",
    "                df[col] = df[col].astype('int32')\n",
    "    \n",
    "    \n",
    "    print(df.dtypes.value_counts())\n",
    "    final_memory_usage = df.memory_usage(deep=True).sum()\n",
    "    print(f\"Final memory usage: {final_memory_usage / 1024**2:.2f} MB\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dataset_version(dataset_version : Literal['1.0','1.1','2.0','3.0']):\n",
    "    \"\"\"\n",
    "    1.0 : Full dataset from our data kernel with Na\n",
    "    1.1 : Full dataset from our data kernel `without` Na (imputed by mean)\n",
    "    2.0 : sampled data, by default 10% of the original data\n",
    "    2.1 : sampled data, by default 10% of the original data `without` Na (imputed by mean) \n",
    "    3.0 : scaled and sampled data without Na, StandardScaler and 10%\n",
    "    4.0 : scaled and PCA\n",
    "    4.1 : scaled and PCA and sampled, by default 10% of the original data size \n",
    "\n",
    "    # How to use:\n",
    "    `X, y, current_version = change_dataset_version('1.0')`\n",
    "    \"\"\"\n",
    "    reset_global_var = ['X','y','current_version']\n",
    "    clear_global_variable(reset_global_var)\n",
    "\n",
    "    if dataset_version == '1.0':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "        y = 1 - y\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        for col in X.select_dtypes(include=['Int64']).columns:\n",
    "            X[col] = X[col].astype('float64')\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "        Xtrain, Xvalid, ytrain, yvalid =  train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "    elif dataset_version == '1.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "        \n",
    "        # Replace negative infinity values (if applicable)\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        #Identify int col:\n",
    "        int_columns = train_data.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        #round the mean for those columns:\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        train_data = optimize_int_memory(train_data)\n",
    "        \n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "        y= 1 - y\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "        Xtrain, Xvalid, ytrain, yvalid =  train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "    elif dataset_version == '2.0':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Sample 10% of the data\n",
    "        train_data = train_data.sample(frac=0.10, random_state=42)\n",
    "        train_data.reset_index(inplace=True)\n",
    "        #Remove characters that model can't read\n",
    "        train_data.columns = train_data.columns.str.replace('[^\\w\\s]','')\n",
    "        #Save the df for dashboard purposes\n",
    "        #train_data.to_csv('./Dashboard/sampled_train_data.csv')\n",
    "        \n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy().astype(int)\n",
    "        y = 1 - y\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        for col in X.select_dtypes(include=['Int64']).columns:\n",
    "            X[col] = X[col].astype('float64')\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "        Xtrain, Xvalid, ytrain, yvalid =  train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "    elif dataset_version == '2.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Sample 10% of the data\n",
    "        train_data = train_data.sample(frac=0.10, random_state=42)\n",
    "\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "\n",
    "        # Replace negative infinity values\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        #Identify int col:\n",
    "        int_columns = train_data.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        #round the mean for those columns:\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        #Restructure the dtypes for memory optimisation\n",
    "        train_data = optimize_int_memory(train_data)\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "        y = 1 - y\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "\n",
    "        Xtrain, Xvalid, ytrain, yvalid =  train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    elif dataset_version == '3.0':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Sample 10% of the data\n",
    "        train_data = train_data.sample(frac=0.10, random_state=42)\n",
    "\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "\n",
    "        # Replace negative infinity values\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        #Identify int col:\n",
    "        int_columns = train_data.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        #round the mean for those columns:\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        #Restructure the dtypes for memory optimisation\n",
    "        train_data = optimize_int_memory(train_data)\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "        y = 1 - y\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        \n",
    "        # Scaling the data.\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled_array = scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(X_scaled_array, columns=X.columns)\n",
    "\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data, X_scaled_array\n",
    "        gc.collect()\n",
    "        Xtrain, Xvalid, ytrain, yvalid =  train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "    elif dataset_version == '4.0':\n",
    "        # Same process as '3.0', but with additional PCA step.\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "        y = 1 - y\n",
    "\n",
    "        # Get the columns to be included in X.\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "\n",
    "        # Replace negative infinity values\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        #Identify int col:\n",
    "        int_columns = train_data.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        #round the mean for those columns:\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        #Restructure the dtypes for memory optimisation\n",
    "        train_data = optimize_int_memory(train_data)\n",
    "\n",
    "        # Scaling the data.\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # Apply PCA.\n",
    "        pca = PCA(n_components=0.95)\n",
    "        X = pca.fit_transform(X)\n",
    "\n",
    "        # Convert back to DataFrame.\n",
    "        X = pd.DataFrame(X, columns=[\"PC\" + str(i) for i in range(1, X.shape[1] + 1)])\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "        Xtrain, Xvalid, ytrain, yvalid =  train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    if dataset_version == '4.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index', 'level_0']\n",
    "        # Get the columns to be included in X.\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "\n",
    "        y = train_data.pop('TARGET')\n",
    "        y = 1 - y\n",
    "        X = train_data\n",
    "\n",
    "        # Data splitting\n",
    "        X, _, y, _ = train_test_split(X, y, stratify=y, test_size=0.9, random_state=42)\n",
    "    \n",
    "        # Replace infinity values more efficiently\n",
    "        inf_mask = X == np.inf\n",
    "        max_values = X[~inf_mask].max()\n",
    "        X.where(~inf_mask, max_values, axis=1, inplace=True)\n",
    "\n",
    "        minus_inf_mask = X == -np.inf\n",
    "        min_values = X[~minus_inf_mask].min()\n",
    "        X.where(~minus_inf_mask, min_values, axis=1,inplace=True)\n",
    "\n",
    "        # Fill NaN values with column means\n",
    "        int_columns = X.select_dtypes(include=['int64', 'int32', 'int8', 'Int64']).columns\n",
    "        column_means = X.mean()\n",
    "        column_means[int_columns] = column_means[int_columns].round()\n",
    "        X.fillna(column_means, inplace=True)\n",
    "\n",
    "        # Memory optimization\n",
    "        X = optimize_int_memory(X)\n",
    "        \n",
    "\n",
    "        # Scaling\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # PCA\n",
    "        pca = PCA(n_components=0.95)\n",
    "        X = pca.fit_transform(X)\n",
    "        X = pd.DataFrame(X, columns=[\"PC\" + str(i) for i in range(1, X.shape[1] + 1)])\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "        Xtrain, Xvalid, ytrain, yvalid =  train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    ratio = y.sum()/len(y)*100\n",
    "    print(\"Target 1-0 ratio: {:.2f}%\".format(ratio))\n",
    "    return Xtrain, ytrain, Xvalid, yvalid, dataset_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_5_rows(X, y):\n",
    "\n",
    "    X_5 = X.head()\n",
    "    y_5 = y.head()\n",
    "\n",
    "    X_5.to_csv('./app/test/X_head', index = False)\n",
    "    y_5.to_csv('./app/test/y_head',index = False)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create functions that will become our baseline for model iteration**\n",
    "\n",
    "After we have our data we will put it through this function and it will get us:\n",
    "- The best hyperparameters for those data and this model\n",
    "- Save all information about the model run so we can compare with other results and pick the best model at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'randomForestClassifier': lambda params: RandomForestClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42),\n",
    "    'lightGBM': lambda params: lgb.LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], n_estimators=params['n_estimators'], random_state=42),\n",
    "    'logisticRegression': lambda params: LogisticRegression(C=params['C'], max_iter=1500, solver='sag'),\n",
    "    'dummyClassifier': lambda params: DummyClassifier(strategy='constant', constant=0),\n",
    "    'fullLGBM' : lambda params: lgb.LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], n_estimators=params['n_estimators'], random_state=42),\n",
    "    #'SVC': lambda params: SVC(C=params['C'], gamma=params['gamma'], probability=True, random_state=42)\n",
    "    # Add more models here as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_name, params, X, y, use_smote = False, threshold = 0.5):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    accuracies = []\n",
    "    adjusted_scores = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    all_y_pred_prob = []  \n",
    "    all_y_true = []\n",
    "    fps=[]\n",
    "    fns=[]\n",
    "\n",
    "\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "\n",
    "        if use_smote:\n",
    "            #smote = SMOTE()\n",
    "            smote = BorderlineSMOTE(kind='borderline-1')\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "            #ratio = y_train.sum()/len(y_train)*100\n",
    "            #print(\"Target 1-0 ratio: {:.2f}%\".format(ratio))\n",
    "            \n",
    "        #Create the model with given param\n",
    "        model = models[model_name](params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        #calculate the pred probabilities on the test set for ROC AUC later\n",
    "        y_pred_ = model.predict_proba(X_val)\n",
    "\n",
    "        # Adjust classification threshold\n",
    "        y_pred = (y_pred_[:,0] < threshold).astype(int)\n",
    "\n",
    "        #Calculate accuracy\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        #Calculate recall and f1_score\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        #accuracy is not everything, in our case errors on FP and FN are vastly different FN are lossing the bank much more money than FP\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "        if use_smote:\n",
    "            adjusted_score =  fn + 10 * fp \n",
    "        else:\n",
    "            adjusted_score =  fn + 90 *fp\n",
    "        \n",
    "        fps.append(fp)    \n",
    "        fns.append(fn)    \n",
    "        adjusted_scores.append(adjusted_score)\n",
    "\n",
    "        all_y_pred_prob.extend(y_pred_[:,1])\n",
    "        all_y_true.extend(y_val)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_adjusted_score = np.mean(adjusted_scores)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "    mean_fp = np.mean(fps)\n",
    "    mean_fn = np.mean(fns)\n",
    "\n",
    "    return model, mean_accuracy, all_y_true, all_y_pred_prob, mean_recall, mean_f1_score, mean_fp, mean_fn, adjusted_scores, mean_adjusted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, params, X, y, use_smote : bool, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    This function is what optuna will try to minimze during the best param search \n",
    "    \"\"\"\n",
    "    _, _, _, _, _, _, _, _, _, adjusted_score= train_and_evaluate_model(model_name, params,X,y,use_smote,threshold)\n",
    "    \n",
    "\n",
    "    return adjusted_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_experiment(model_name : Literal['lightGBM','randomForestClassifier','logisticRegression','dummyClassifier'],\n",
    "                           X, y, max_trials:int, use_smote = False):\n",
    "    \n",
    "    #define the pruner\n",
    "    pruner = MedianPruner(n_startup_trials=1, n_warmup_steps=5, interval_steps=1)\n",
    "    # Create an Optuna study object\n",
    "    study = optuna.create_study(direction='minimize', pruner=pruner)  # 'minimize' for custom \"métier\" score\n",
    "\n",
    "    # Optimize the objective function (number of trials specified by max_trials)\n",
    "    study.optimize(lambda trial: objective(model_name, \n",
    "                                       {'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                                        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "                                       } if model_name == 'randomForestClassifier' else\n",
    "\n",
    "                                       {'num_leaves': trial.suggest_int('num_leaves', 31, 150),\n",
    "                                        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "                                        'n_estimators': trial.suggest_int('n_estimators', 100, 600)\n",
    "                                       } if model_name == 'lightGBM' else\n",
    "                                       {\n",
    "                                        'C': trial.suggest_float('C', 0.001, 100)\n",
    "                                        } if model_name == 'logisticRegression' else\n",
    "                                       {   \n",
    "                                       }if model_name == 'dummyClassifier'else\n",
    "                                        # Add more hyperparameters as needed\n",
    "                                       {\n",
    "                                        #Write Hparams for other models\n",
    "                                       }, \n",
    "                                       X, y, use_smote, threshold = trial.suggest_float('threshold', 0.1, 0.6)), n_trials=max_trials)\n",
    "\n",
    "    # Retrieve the best hyperparameters from the study\n",
    "    best_params = study.best_params\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_best_model(model_name : Literal['lightGBM','randomForestClassifier','logisticRegression','dummyClassifier'],\n",
    "                    best_params, X, y, dataset_version:str, use_smote = False, use_shap = False):\n",
    "    \n",
    "        # Start an MLflow run to track the training process\n",
    "    with mlflow.start_run():\n",
    "        # Log the dataset version\n",
    "        mlflow.log_param('dataset_version', dataset_version)\n",
    "        mlflow.log_param('SMOTE', use_smote)\n",
    "        mlflow.log_param('model type', model_name)\n",
    "\n",
    "        # Log the hyperparameters\n",
    "        mlflow.log_params(best_params)\n",
    "        threshold = best_params['threshold']\n",
    "        # Train and evaluate the model using the best hyperparameters\n",
    "        model, accuracy, all_y_true, y_pred_prob, recall, f1_score,fp, fn, adjusted_scores, adjusted_score = train_and_evaluate_model(model_name, best_params, X, y, use_smote,threshold)\n",
    "\n",
    "        # Log the metrics\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        mlflow.log_metric('recall', recall)\n",
    "        mlflow.log_metric('false_positive', fp)\n",
    "        mlflow.log_metric('false_negative', fn)\n",
    "        mlflow.log_metric('f1_score', f1_score)\n",
    "        mlflow.log_metric('adjusted_score', adjusted_score)\n",
    "        for i, score in enumerate(adjusted_scores, 1):\n",
    "            mlflow.log_metric(f'adjusted_score_{i}', score)\n",
    "\n",
    "        # Save the model as an artifact\n",
    "        mlflow.sklearn.log_model(model, 'model')\n",
    "\n",
    "        #region ROC AUC\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(all_y_true, y_pred_prob)\n",
    "        mlflow.log_metric('roc_auc', roc_auc)\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(all_y_true, y_pred_prob)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        roc_curve_img_path = './Exports/roc_curve.png'\n",
    "        plt.savefig(roc_curve_img_path)\n",
    "\n",
    "        # Log the ROC curve image as an artifact\n",
    "        mlflow.log_artifact(roc_curve_img_path)\n",
    "\n",
    "        # Close the plot to free up memory\n",
    "        plt.close()\n",
    "        # endregion \n",
    "\n",
    "        #region Shap\n",
    "        if use_shap:            \n",
    "            #local importance:\n",
    "            index = 1\n",
    "\n",
    "            if model_name == 'dummyClassifier':\n",
    "                return  \n",
    "            elif model_name == 'logisticRegression':\n",
    "                explainer = shap.Explainer(model, X)\n",
    "                shap_values = explainer(X)\n",
    "                shap_html = shap.plots.force(shap_values[index], show=False)\n",
    "                html_filepath = f\"./Exports/shap_force_plot_{index}.html\"\n",
    "                shap.save_html(html_filepath, shap_html)\n",
    "\n",
    "            else: #TreeBased model\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X)\n",
    "                shap_html = shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X.iloc[0,:], show=False)\n",
    "                # Save the HTML content to a file\n",
    "                html_filepath = f\"./Exports/shap_force_plot_{index}.html\"\n",
    "                shap.save_html(html_filepath, shap_html)\n",
    "\n",
    "            # Now for the full model feature importance\n",
    "            shap.summary_plot(shap_values, X, show=False)\n",
    "            plt.savefig('./Exports/shap_summary.png')\n",
    "    \n",
    "            # Log the SHAP force plot and summary plot as artifacts\n",
    "            mlflow.log_artifact(html_filepath)\n",
    "            mlflow.log_artifact('./Exports/shap_summary.png')\n",
    "        #endregion\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(Xvalid, yvalid, model,best_params):\n",
    "    threshold = best_params['threshold']\n",
    "    \n",
    "\n",
    "    y_pred_prob = model.predict_proba(Xvalid)\n",
    "    y_pred_prob_0 = y_pred_prob[:,0]\n",
    "    y_pred_prob_1 = y_pred_prob[:,1]\n",
    "    # Adjust classification threshold\n",
    "    y_pred = (y_pred_prob_0 < threshold).astype(int)\n",
    "    accuracy = accuracy_score(yvalid, y_pred)\n",
    "    \n",
    "    #Calculate recall and f1_score\n",
    "    recall = recall_score(yvalid, y_pred)\n",
    "    \n",
    "    f1 = f1_score(yvalid, y_pred)\n",
    "\n",
    "    roc_auc = roc_auc_score(yvalid, y_pred_prob_1)\n",
    "\n",
    "    print('accuracy: ', accuracy)\n",
    "    print('roc_auc: ', roc_auc)\n",
    "    print('recall: ', recall)\n",
    "    print('f1 score: ', f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start the mlflow server to see the dashboard //// Be sure to change your directory to the app folder\n",
    "#mlflow server --backend-store-uri sqlite:///mlflow.db\n",
    "\n",
    "# Check if the experiment already exists\n",
    "mlflow.set_tracking_uri('sqlite:///app/mlflow.db')\n",
    "\n",
    "experiment_id = mlflow.get_experiment_by_name('credit_models')\n",
    "if experiment_id is None:\n",
    "    mlflow.create_experiment('credit_models', artifact_location='./app/mlruns/1')\n",
    "    print('Experiment created')\n",
    "else:\n",
    "    print(f\"Experiment 'credit_models' already exists with id {experiment_id.experiment_id}.\")\n",
    "    mlflow.set_experiment('credit_models')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, Xvalid, yvalid, current_version = change_dataset_version('2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to save some data for the model you put in production to test it \n",
    "save_5_rows(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_optuna_params = run_optuna_experiment('logisticRegression',X,y,10,use_smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optuna_params = {'C': 49.11321757734528, 'threshold': 0.40447759968727737}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run_best_model('logisticRegression',best_optuna_params,X,y,current_version,use_smote=True,use_shap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate model\n",
    "validate_model(Xvalid, yvalid, model, best_optuna_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "**Let's see how the predict part is going to look like**\n",
    "\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.head(30)\n",
    "y1 = y.head(30)\n",
    "\n",
    "#set the current model ID and load the model f4f4d1af1ed34f8d9497b2ce38e9581e eacb0575c389406783c47125c5f3ce85\n",
    "# Winner : 69c6dfea00f44d549419e7de4cf5262c\n",
    "model_id = '69c6dfea00f44d549419e7de4cf5262c'\n",
    "model_uri = f\"./app/mlruns/1/{model_id}/artifacts/model\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "mlflow_client = mlflow.tracking.MlflowClient(tracking_uri='http://0.0.0.0:5000')\n",
    "run = mlflow_client.get_run(run_id=model_id)\n",
    "threshold = run.data.params['threshold']\n",
    "threshold = float(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>...</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_Signed_MAX</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_Signed_MEAN</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_Signed_SUM</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_Signed_VAR</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_MIN</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_MAX</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_MEAN</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_SUM</th>\n",
       "      <th>CC_NAME_CONTRACT_STATUS_nan_VAR</th>\n",
       "      <th>CC_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15168</th>\n",
       "      <td>130054.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>469147.5</td>\n",
       "      <td>22698.0</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>233135.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>814041.0</td>\n",
       "      <td>23931.0</td>\n",
       "      <td>679500.0</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>268242.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>1546020.0</td>\n",
       "      <td>45202.5</td>\n",
       "      <td>1350000.0</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13014</th>\n",
       "      <td>59973.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>677664.0</td>\n",
       "      <td>43438.5</td>\n",
       "      <td>585000.0</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24547</th>\n",
       "      <td>28201.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>15750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 796 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        level_0  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
       "15168  130054.0            1             0                1             0   \n",
       "515    233135.0            1             0                0             2   \n",
       "7031   268242.0            0             1                0             1   \n",
       "13014   59973.0            0             1                0             0   \n",
       "24547   28201.0            1             0                0             0   \n",
       "\n",
       "       AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "15168          112500.0    469147.5      22698.0         351000.0   \n",
       "515            112500.0    814041.0      23931.0         679500.0   \n",
       "7031           337500.0   1546020.0      45202.5        1350000.0   \n",
       "13014          351000.0    677664.0      43438.5         585000.0   \n",
       "24547           45000.0    315000.0      15750.0              NaN   \n",
       "\n",
       "       REGION_POPULATION_RELATIVE  ...  CC_NAME_CONTRACT_STATUS_Signed_MAX  \\\n",
       "15168                    0.035792  ...                                 0.0   \n",
       "515                      0.018634  ...                                 NaN   \n",
       "7031                     0.008019  ...                                 0.0   \n",
       "13014                    0.018634  ...                                 NaN   \n",
       "24547                    0.019101  ...                                 NaN   \n",
       "\n",
       "       CC_NAME_CONTRACT_STATUS_Signed_MEAN  \\\n",
       "15168                                  0.0   \n",
       "515                                    NaN   \n",
       "7031                                   0.0   \n",
       "13014                                  NaN   \n",
       "24547                                  NaN   \n",
       "\n",
       "       CC_NAME_CONTRACT_STATUS_Signed_SUM  CC_NAME_CONTRACT_STATUS_Signed_VAR  \\\n",
       "15168                                 0.0                                 0.0   \n",
       "515                                   NaN                                 NaN   \n",
       "7031                                  0.0                                 0.0   \n",
       "13014                                 NaN                                 NaN   \n",
       "24547                                 NaN                                 NaN   \n",
       "\n",
       "       CC_NAME_CONTRACT_STATUS_nan_MIN  CC_NAME_CONTRACT_STATUS_nan_MAX  \\\n",
       "15168                              0.0                              0.0   \n",
       "515                                NaN                              NaN   \n",
       "7031                               0.0                              0.0   \n",
       "13014                              NaN                              NaN   \n",
       "24547                              NaN                              NaN   \n",
       "\n",
       "       CC_NAME_CONTRACT_STATUS_nan_MEAN  CC_NAME_CONTRACT_STATUS_nan_SUM  \\\n",
       "15168                               0.0                              0.0   \n",
       "515                                 NaN                              NaN   \n",
       "7031                                0.0                              0.0   \n",
       "13014                               NaN                              NaN   \n",
       "24547                               NaN                              NaN   \n",
       "\n",
       "       CC_NAME_CONTRACT_STATUS_nan_VAR  CC_COUNT  \n",
       "15168                              0.0      47.0  \n",
       "515                                NaN       NaN  \n",
       "7031                               0.0      93.0  \n",
       "13014                              NaN       NaN  \n",
       "24547                              NaN       NaN  \n",
       "\n",
       "[5 rows x 796 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel:\n",
    "    def __init__(self, mlflow_model):\n",
    "        self.model = mlflow_model\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(self.model._model_impl, \"predict_proba\"):\n",
    "            return self.model._model_impl.predict_proba(X)\n",
    "        else:\n",
    "            raise AttributeError(\"Underlying model does not have a 'predict_proba' method.\")\n",
    "\n",
    "wrapped_model = WrappedModel(model)\n",
    "proba = wrapped_model.predict_proba(X1)\n",
    "prediction = (proba[:,0] < threshold).astype(int)\n",
    "\n",
    "\n",
    "api_answers = []\n",
    "for i in range(0, len(prediction)):\n",
    "    api_answer = [prediction[i], proba[i][0], proba[i][1]]\n",
    "    api_answers.append(api_answer)\n",
    "display(api_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
