{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "#Data pre processing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Scoring\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#Hyperparam opti\n",
    "import optuna\n",
    "\n",
    "#Mlflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=AttributeError)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_global_variable(variable_name):\n",
    "    \"\"\"\n",
    "    Check if a global variable exists and delete it if it does.\n",
    "    \"\"\"\n",
    "    for v in variable_name:\n",
    "        if v in globals():\n",
    "            # Delete the global variable\n",
    "            del globals()[v]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dataset_version(dataset_version : Literal['1.0','1.1','2.0','3.0']):\n",
    "    \"\"\"\n",
    "    1.0 : Full dataset from our data kernel with Na\n",
    "    1.1 : Full dataset from our data kernel `without` Na (imputed by mean)\n",
    "    2.0 : sampled data, by default 10% of the original data\n",
    "    2.1 : sampled data, by default 10% of the original data `without` Na (imputed by mean)\n",
    "    3.0 : scaled data\n",
    "    4.0 : scaled and PCA\n",
    "    4.1 : scaled and PCA and sampled, by default 10% of the original data size \n",
    "\n",
    "    # How to use:\n",
    "    `X, y, current_version = change_dataset_version('1.0')`\n",
    "    \"\"\"\n",
    "    reset_global_var = ['X','y','current_version']\n",
    "    clear_global_variable(reset_global_var)\n",
    "\n",
    "    if dataset_version == '1.0':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '1.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "        \n",
    "        # Replace negative infinity values (if applicable)\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '2.0':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Sample 10% of the data\n",
    "        train_data = train_data.sample(frac=0.10, random_state=42)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '2.1':\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        # Sample 10% of the data\n",
    "        train_data = train_data.sample(frac=0.10, random_state=42)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = train_data.mean()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        train_data.fillna(column_means, inplace=True)\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in train_data.columns:\n",
    "            max_val = train_data[train_data[col] != np.inf][col].max()\n",
    "            train_data[col].replace(np.inf, max_val, inplace=True)\n",
    "\n",
    "        # Replace negative infinity values (if applicable)\n",
    "        for col in train_data.columns:\n",
    "            min_val = train_data[train_data[col] != -np.inf][col].min()\n",
    "            train_data[col].replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "        #Remove characters that model can't read\n",
    "        X.columns = X.columns.str.replace('[^\\w\\s]','')\n",
    "\n",
    "        #Clean unused data to free up ram\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    elif dataset_version == '3.0':\n",
    "        # Same process as '1.0', but with additional scaling step.\n",
    "        # Load the data.\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X.\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "\n",
    "        # Scaling the data.\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '4.0':\n",
    "        # Same process as '3.0', but with additional PCA step.\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X.\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "\n",
    "        # Scaling the data.\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # Apply PCA.\n",
    "        pca = PCA(n_components=0.95)\n",
    "        X = pca.fit_transform(X)\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "    elif dataset_version == '4.1':\n",
    "        # Same process as '4.0', but with additional sampling step.\n",
    "        train_data = pd.read_feather(\"./train_data.feather\")\n",
    "        not_usable_col = ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']\n",
    "        train_data.drop(columns='level_0', inplace=True)\n",
    "\n",
    "        #y is just the target column\n",
    "        y = train_data['TARGET'].copy()\n",
    "\n",
    "        # Get the columns to be included in X.\n",
    "        columns_X = [col for col in train_data.columns if col not in not_usable_col]\n",
    "        X = train_data[columns_X]\n",
    "\n",
    "        X, _, y, _ = train_test_split(X, y, stratify=y, test_size=0.9, random_state=42)\n",
    "\n",
    "        # Calculate the mean of each column\n",
    "        column_means = X.mean()\n",
    "        # Fill NaN values in each column with the corresponding mean value\n",
    "        X.fillna(column_means, inplace=True)\n",
    "\n",
    "        # Replace positive infinity values\n",
    "        for col in X.columns:\n",
    "            max_val = X[X[col] != np.inf][col].max()\n",
    "            X[col].replace(np.inf, max_val, inplace=True)\n",
    "\n",
    "        # Scaling the data.\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # Apply PCA.\n",
    "        pca = PCA(n_components=0.95)\n",
    "        X = pca.fit_transform(X)\n",
    "\n",
    "        # Convert back to DataFrame.\n",
    "        X = pd.DataFrame(X, columns=[\"PC\" + str(i) for i in range(1, X.shape[1] + 1)])\n",
    "\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    ratio = y.sum()/len(y)*100\n",
    "    print(\"Target 1-0 ratio: {:.2f}%\".format(ratio))\n",
    "    return X, y, dataset_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_5_rows(X, y):\n",
    "\n",
    "    X_5 = X.head()\n",
    "    y_5 = y.head()\n",
    "\n",
    "    X_5.to_csv('./test_df/X_head', index = False)\n",
    "    y_5.to_csv('./test_df/y_head',index = False)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create functions that will become our baseline for model iteration**\n",
    "\n",
    "After we have our data we will put it through this function and it will get us:\n",
    "- The best hyperparameters for those data and this model\n",
    "- Save all information about the model run so we can compare with other results and pick the best model at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'randomForestClassifier': lambda params: RandomForestClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'], random_state=42),\n",
    "    'lightGBM': lambda params: lgb.LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], n_estimators=params['n_estimators'], random_state=42),\n",
    "    'logisticRegression': lambda params: LogisticRegression(C=params['C'], random_state=42, max_iter=1000)\n",
    "    #'SVC': lambda params: SVC(C=params['C'], gamma=params['gamma'], probability=True, random_state=42)\n",
    "    # Add more models here as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_name, params, X, y, use_smote = False):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    accuracies = []\n",
    "    adjusted_scores = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    all_y_pred_prob = []\n",
    "    all_y_true = []  # <--- List for all true labels in validation sets\n",
    "\n",
    "\n",
    "    for train_index, val_index in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "\n",
    "        if use_smote:\n",
    "            smote = SMOTE()\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "            #ratio = y_train.sum()/len(y_train)*100\n",
    "            #print(\"Target 1-0 ratio: {:.2f}%\".format(ratio))\n",
    "            \n",
    "        #Create the model with given param\n",
    "        model = models[model_name](params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        #calculate the pred probabilities on the test set for ROC AUC later\n",
    "        y_pred_prob = model.predict_proba(X_val)[:,1]\n",
    "\n",
    "        #Calculate accuracy\n",
    "        y_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        #Calculate recall and f1_score\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        #accuracy is not everything, in our case errors on FP and FN are vastly different FN are lossing the bank much more money than FP\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "        adjusted_score = 10 * fn + fp\n",
    "        adjusted_scores.append(adjusted_score)\n",
    "\n",
    "        all_y_pred_prob.extend(y_pred_prob)\n",
    "        all_y_true.extend(y_val)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_adjusted_score = np.mean(adjusted_scores)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    return model, mean_accuracy, all_y_true, all_y_pred_prob, mean_recall, mean_f1_score, adjusted_scores, mean_adjusted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model_name, params, X, y, use_smote : bool):\n",
    "    \"\"\"\n",
    "    This function is what optuna will try to minimze during the best param search \n",
    "    \"\"\"\n",
    "    _, _, _, _, _, _, _, adjusted_score= train_and_evaluate_model(model_name, params,X,y,use_smote)\n",
    "    \n",
    "\n",
    "    return adjusted_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_experiment(model_name : Literal['lightGBM','randomForestClassifier','logisticRegression'],\n",
    "                           X, y, max_trials:int, use_smote = False):\n",
    "    \n",
    "    # Create an Optuna study object\n",
    "    study = optuna.create_study(direction='minimize')  # 'minimize' for custom \"mÃ©tier\" score\n",
    "\n",
    "    # Optimize the objective function (number of trials specified by max_trials)\n",
    "    study.optimize(lambda trial: objective(model_name, \n",
    "                                       {'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                                        'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "                                       } if model_name == 'randomForestClassifier' else\n",
    "\n",
    "                                       {'num_leaves': trial.suggest_int('num_leaves', 31, 50),\n",
    "                                        'max_depth': trial.suggest_int('max_depth', -1, 50),\n",
    "                                        'n_estimators': trial.suggest_int('n_estimators', 100, 200)\n",
    "                                       } if model_name == 'lightGBM' else\n",
    "                                       {'C': trial.suggest_float('C', 0.1, 10)\n",
    "                                        # Add more hyperparameters as needed\n",
    "                                       } if model_name == 'logisticRegression' else\n",
    "\n",
    "                                       {\n",
    "                                        #Write Hparams for other models\n",
    "                                       }, \n",
    "                                       X, y, use_smote), n_trials=max_trials)\n",
    "\n",
    "    # Retrieve the best hyperparameters from the study\n",
    "    best_params = study.best_params\n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_best_model(model_name : Literal['lightGBM','randomForestClassifier','logisticRegression'],\n",
    "                    best_params, X, y, dataset_version:str, use_smote = False, shap = False):\n",
    "    \n",
    "        # Start an MLflow run to track the training process\n",
    "    with mlflow.start_run():\n",
    "        # Log the dataset version\n",
    "        mlflow.log_param('dataset_version', dataset_version)\n",
    "        mlflow.log_param('SMOTE', use_smote)\n",
    "        mlflow.log_param('model type', model_name)\n",
    "\n",
    "        # Log the hyperparameters\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Train and evaluate the model using the best hyperparameters\n",
    "        model, accuracy, all_y_true, y_pred_prob, recall, f1_score, adjusted_scores, adjusted_score = train_and_evaluate_model(model_name, best_params, X, y, use_smote)\n",
    "\n",
    "        # Log the metrics\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        mlflow.log_metric('recall', recall)\n",
    "        mlflow.log_metric('f1_score', f1_score)\n",
    "        mlflow.log_metric('adjusted_score', adjusted_score)\n",
    "        mlflow.log_metrics('adjusted_scores_CV', adjusted_scores)\n",
    "\n",
    "        # Save the model as an artifact\n",
    "        mlflow.sklearn.log_model(model, 'model')\n",
    "\n",
    "        #region ROC AUC\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(all_y_true, y_pred_prob)\n",
    "        mlflow.log_metric('roc_auc', roc_auc)\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(all_y_true, y_pred_prob)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        roc_curve_img_path = './Exports/roc_curve.png'\n",
    "        plt.savefig(roc_curve_img_path)\n",
    "\n",
    "        # Log the ROC curve image as an artifact\n",
    "        mlflow.log_artifact(roc_curve_img_path)\n",
    "\n",
    "        # Close the plot to free up memory\n",
    "        plt.close()\n",
    "        # endregion \n",
    "\n",
    "        #region Shap\n",
    "        if shap:\n",
    "            # Calculate SHAP values\n",
    "            explainer = shap.Explainer(model, X)\n",
    "            shap_values = explainer(X, check_additivity=False)\n",
    "\n",
    "            # Create a summary plot and save it\n",
    "            shap.summary_plot(shap_values, X, show=False)\n",
    "            plt.savefig('./Exports/shap_summary.png')\n",
    "\n",
    "            # Log the SHAP summary plot as an artifact\n",
    "            mlflow.log_artifact('./Exports/shap_summary.png')\n",
    "        #endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('sqlite:///app/mlflow.db')\n",
    "mlflow.set_experiment('credits_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, current_version = change_dataset_version('4.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_5_rows(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optuna_params = run_optuna_experiment('lightGBM',X,y,5,use_smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_best_model('lightGBM',best_optuna_params,X,y,current_version,use_smote=False,shap=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
